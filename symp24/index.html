<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">

        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <title>SoCal NLP Symposium 2024</title>
        <meta name="description" content="Welcome to the Southern California Natural Language Processing Symposium 2024!">
        <meta name="keywords" content="SoCal,NLP,Symposium,Southern California,Natural Language Processing">
        <meta name="author" content="SoCal NLP Symposium Organizers">

        <link rel="icon" href="static/favicon.ico">

        <link rel="stylesheet" href="css/bootstrap.min.css">
        <link rel="stylesheet" href="css/fonts.css">
        <link rel="stylesheet" href="css/custom.css">
    </head>

    <body>
        <div id="header">
            <div class="container">
               <div id="header-desc">
                   <p style="font-size:2.2rem;text-align:center;">SoCal NLP Symposium 2024</p>
                   <p style="font-size:1.3rem;text-align:center;">University of California, San Diego | Nov 22, 2024</p>
               </div>
            </div>
        </div>

        <div class="container">
            <nav class="navbar navbar-expand-lg navbar-light bg-light rounded">
                <a class="navbar-brand" href="index.html">Welcome</a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#content" aria-controls="navbarsExample09" aria-expanded="false" aria-label="Toggle navigation">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <div class="collapse navbar-collapse" id="content">
                    <ul class="navbar-nav mr-auto">
                        <!--<li class="nav-item">-->
                            <!--<a class="nav-link" href="#cfp"><b>Call For Papers</b></a>-->
                        <!--</li>-->
                        <li class="nav-item">
                            <a class="nav-link" href="#venue"><b>Venue</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#schedule"><b>Schedule</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#speaker"><b>Invited Speakers</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="#paper"><b>Accepted Work</b></a>
                        </li>
                        
                        <li class="nav-item">
                            <a class="nav-link" href="#organizer"><b>Organizers</b></a>
                        </li>

                        <li class="nav-item">
                            <a class="nav-link" href="#sponsor"><b>Sponsors</b></a>
                        </li>

                        <li class="nav-item">
                            <a class="nav-link" href='#past'><b>Past Symposiums</b></a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href='#contact'><b>Contact</b></a>
                        </li>
                    </ul>
                </div>
            </nav>
        </div>

        <br/>

        <div id="welcome" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Welcome</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>The goal of the Southern California Natural Language Processing Symposium is to gather researchers from the southern California region with broad expertise in machine learning and natural language processing. The symposium will provide a participatory environment where attendees from a variety of fields can share and discuss their latest findings.
                    </p>
                    <h5>Important Dates</h5>
                    <p>
                        <b>Submission deadline</b>: Oct 26 (Saturday), 2024, 11:59 PM Anywhere On Earth <a href="https://openreview.net/group?id=SoCalNLP/2024/Symposium">[<b>OpenReview Submission Portal</b>]</a><br>
                        <b>Acceptance notification</b>: Nov. 1, 2024<br/>
                        <b>Registration deadline</b>: Nov 10, 2024 <a href="https://www.eventbrite.com/e/socal-nlp-2024-tickets-1046390150107">[<b>Eventbrite Registration Link</b>]</a><br/>
                        <b>Symposium date</b>: Nov 22, 2024 (Friday)<br/>
                        <b>Submission format</b>: 
                        <ul>
                            <li>If the paper is published in a recent conference or journal, you can directly submit the paper without modification. <strong>Please indicate where the paper is published in the Abstract when submitting the paper</strong>. </li>
                            <li>For papers that have not been previously published, we recommend submitting an extended summary spanning no more than two pages, formatted according to <a href="https://github.com/acl-org/acl-style-files">the ACL guidelines</a>, However, submissions of a longer length will also be considered.</li>
                            <li> All submissions are single-blind reviewed. 
                        </ul>
                    </p>
                </div>
            </div>
        </div>

        <br/>

        <div id="venue" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Venue</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>
                        <b>Date:</b> Nov 22, 2024<br/><br/>
                        <b>Locations:</b> There are <b>two venues</b> involved in this event. They are close to each other.
                        <p>
                        Breakfast, Lunch, and Poster: <b>Great Hall</b><br/>
                        <iframe src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d2529.2780678773297!2d-117.24087998098204!3d32.88398617564115!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x80dc06ea9d2990e7%3A0x1b74b3338726c0ff!2sGreat%20Hall!5e0!3m2!1sen!2sus!4v1728838443680!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
                        </p>
                        <br/><br/>
                        Talk: <b>Institute of the America's Hojel Auditorium</b><br/>
                        <iframe src="https://www.google.com/maps/embed?pb=!1m14!1m8!1m3!1d3350.4960830433806!2d-117.2439519!3d32.8850498!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x80dc06ea8b05d07b%3A0xe8e6c3b00a1a3e4e!2sHojel%20Auditorium!5e0!3m2!1sen!2sus!4v1728838354922!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
                        <br/><br/><br/>
                        <b>Parking information:</b>
                        Campus parking is very limited and is not recommended. You can check available options here. The closest parking lot for visitors is located on the top floor of <a href="https://maps.app.goo.gl/FfjaukxKu9E3Kfaw9">Hopkins Parking Structure</a>. You can pay the fee in pay station or pay-by-phone. Note that most visitor spots are limited to two hours. Even though the app allows you to pay for longer periods, you will get a ticket after that time.
                        <br/>
                        <iframe src="https://www.google.com/maps/embed?pb=!1m18!1m12!1m3!1d3350.544127048149!2d-117.24197752354003!3d32.88377907361924!2m3!1f0!2f0!3f0!3m2!1i1024!2i768!4f13.1!3m3!1m2!1s0x80dc06c1e6156103%3A0xc6690b999c002d97!2sHopkins%20Parking!5e0!3m2!1sen!2sus!4v1728838528910!5m2!1sen!2sus" width="600" height="450" style="border:0;" allowfullscreen="" loading="lazy" referrerpolicy="no-referrer-when-downgrade"></iframe>
                        <br/><br/>
                        There is <b>free parking</b> at <a href="https://www.flytorrey.com/">Torrey Pines Gliderport</a>, if you do not mind a 30-minute walk to the venue. The view from the Gliderport is spectacular.
                        <br/>
                        <a href="https://www.flickr.com/photos/ginatrapani/2840540583/">
                        <img fetchpriority="high" decoding="async" width="800" height="600" src="static/glider.jpeg"></a><figcaption class="wp-element-caption">View from the Torrey Pines Gliderport. Photo credit: <a href="https://www.flickr.com/photos/ginatrapani/" title="">Gina Trapani</a> (<a href="https://creativecommons.org/licenses/by-nc-sa/2.0/" title="">CC BY-NC-SA 2.0 Deed</a>)</figcaption>    
                        <br/>
                    </p>
                </div>
            </div>
        </div>

        <br/>

        <div id="schedule" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Schedule</h4>
                    <hr>
                </div>
            </div>
            <p>

<b>Great Hall</b><br/>
09:00am - 09:45am	<i>Breakfast, Registration & Social</i><br/><br/>

<b>Institute of the America's Hojel Auditorium</b><br/>
09:45am - 10:00am	<i>Opening Remarks</i> <br/>
10:00am - 10:40am   <b>Heng Ji</b> (UIUC)<br/>
<b><a href="#Heng">Towards Knowledgeable Foundation Models</a></b><br/>
10:40pm - 11:20am   <b>Eunsol Choi</b> (NYU)<br/>
<b><a href="#Eunsol">Equipping LLMs for Complex Knowledge Scenarios: Interaction and Retrieval</a></b><br/>
11:20am - 11:30am   <i>Mini Break</i><br/><br/>

<b>Great Hall</b><br/>
11:30am - 12:30pm	Poster Session 1<br>
12:00pm - 02:00pm	<i>Lunch Reception & AIX-sponsored Social Hours</i> <br/>
01:30pm - 02:30pm	Poster Session 2 <br/>
02:30pm - 02:40pm   <i>Mini Break</i><br/>
02:40pm - 03:40pm   Poster Session 3 <br/><br/>

<b>Institute of the America's Hojel Auditorium</b><br>
04:00pm - 04:40pm	<b>Dawei Huang</b> (SambaNova.ai)<br/>
<b><a href="#Dawei">Enabling extremely fast inference and training performance using dataflow and custom chip</a></b><br/>
04:40pm - 05:20pm	<b>Taylor Berg-Kirkpatrick</b> (UCSD)<br/>
<b><a href="#Taylor">Large Language Models and Cybersecurity: New Threats and Safeguards</a></b><br/>
05:20pm - 05:30pm   <i>Closing Remarks & Awards</i><br/>
            </p>
        </div>

        <br/>

        <div id="speaker" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Invited Speakers</h4>
                    <hr>
                </div>
            </div>

            <div class="row">
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <figure style="margin: auto;" class="rounded">
                      <a href="https://blender.cs.illinois.edu/hengji.html" target="_block">
                          <img src="https://blender.cs.illinois.edu/HengJi_3.JPG" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </figure>
                  <p class="text-center caption"><b><a href="https://blender.cs.illinois.edu/hengji.html" target="_block">Heng Ji</a></b></p>
                  <p class="text-center caption caption-role"><b>Professor</b></p>
                  <p class="text-center caption caption-role"><b>Computer Science</b></p>
                  <p class="text-center caption caption-loc"><i>UIUC</i></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <figure style="margin: auto;" class="rounded">
                      <a href="https://eunsol.github.io/" target="_block">
                          <img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=6wulN88AAAAJ&citpid=5" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </figure>
                  <p class="text-center caption"><b><a href="https://eunsol.github.io/" target="_block">Eunsol Choi</a></b></p>
                  <p class="text-center caption caption-role"><b>Assistant Professor</b></p>
                  <p class="text-center caption caption-role"><b>Computer Science and Data Science</b></p>
                  <p class="text-center caption caption-loc"><i>NYU</i></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <figure style="margin: auto;" class="rounded">
                      <a href="https://www.linkedin.com/in/dawei-huang-8803b25/" target="_block">
                          <img src="https://media.licdn.com/dms/image/v2/C4E03AQH4Cnuc0RMzTg/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1517740671115?e=1737590400&v=beta&t=5BYjz1q4Qi_AbGHEzBopV1042S1BcQlzhRcP-zyUi3M" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </figure>
                  <p class="text-center caption"><b><a href="https://www.linkedin.com/in/dawei-huang-8803b25/" target="_block">Dawei Huang</a></b></p>
                  <p class="text-center caption caption-role"><b>Senior Director of Engineering</b></p>
                  <p class="text-center caption caption-role"><b>SambaNova Systems</b></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                  <figure style="margin: auto;" class="rounded">
                      <a href="https://cseweb.ucsd.edu/~tberg/" target="_block">
                          <img src="https://cseweb.ucsd.edu/~tberg/photos/bio_pic_2.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                      </a>
                  </figure>
                  <p class="text-center caption"><b><a href="https://cseweb.ucsd.edu/~tberg/" target="_block">Taylor Berg-Kirkpatrick</a></b></p>
                  <p class="text-center caption caption-role"><b>Associate Professor</b></p>
                  <p class="text-center caption caption-role"><b>Computer Science</b></p>
                  <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
            </div>
            <br/>
            <br/>
                    <h5 id="Heng">Heng Ji</h5>
                    <b>Title:</b> Towards Knowledgeable Foundation Models<br/><br/>
                    <b>Abstract:</b> Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance on knowledge reasoning tasks, owing to their implicit knowledge derived from extensive pretraining data. However, their inherent knowledge bases often suffer from disorganization and illusion, bias towards common entities, and rapid obsolescence. Consequently, LLMs frequently make up untruthful information, exhibit resistance to updating outdated knowledge, or struggle with generalizing across multiple languages. In this talk I will discuss several research directions that aim to make foundation models’ knowledge more accurate, organized, up-to-date and fair: (1) Where and How is Knowledge Stored in LLM? (2) How to Control LLM’s Knowledge? (3) How to Update LLM’s Dynamic Knowledge? (4) How to Bridge the Knowledge Gap between Natural Language and Unnatural Language?<br/><br/>
                    <b>Bio:</b> Heng Ji is a professor at Siebel School of Computing and Data Science, and an affiliated faculty member at Electrical and Computer Engineering Department, Coordinated Science Laboratory, and Carl R. Woese Institute for Genomic Biology of University of Illinois Urbana-Champaign. She is an Amazon Scholar. She is the Founding Director of Amazon-Illinois Center on AI for Interactive Conversational Experiences (AICE). She received her B.A. and M. A. in Computational Linguistics from Tsinghua University, and her M.S. and Ph.D. in Computer Science from New York University. Her research interests focus on Natural Language Processing, especially on Multimedia Multilingual Information Extraction, Knowledge-enhanced Large Language Models and Vision-Language Models, and AI for Science. The awards she received include Outstanding Paper Award at ACL2024, two Outstanding Paper Awards at NAACL2024, "Young Scientist" by the World Laureates Association in 2023 and 2024, "Young Scientist" and a member of the Global Future Council on the Future of Computing by the World Economic Forum in 2016 and 2017, "Women Leaders of Conversational AI" (Class of 2023) by Project Voice, "AI's 10 to Watch" Award by IEEE Intelligent Systems in 2013, NSF CAREER award in 2009, PACLIC2012 Best paper runner-up, "Best of ICDM2013" paper award, "Best of SDM2013" paper award, ACL2018 Best Demo paper nomination, ACL2020 Best Demo Paper Award, NAACL2021 Best Demo Paper Award, Google Research Award in 2009 and 2014, IBM Watson Faculty Award in 2012 and 2014 and Bosch Research Award in 2014-2018. She served as the associate editor for IEEE/ACM Transaction on Audio, Speech, and Language Processing, and the Program Committee Co-Chair of many conferences including NAACL-HLT2018 and AACL-IJCNLP2022. She was elected as the North American Chapter of the Association for Computational Linguistics (NAACL) secretary 2020-2023. <br/><br/><br/>

                    <h5 id="Eunsol">Eunsol Choi</h5>
                    <b>Title:</b> Equipping LLMs for Complex Knowledge Scenarios: Interaction and Retrieval<br/><br/>
                    <b>Abstract:</b> Language models are increasingly used as an interface to gather information. Yet trusting the answers generated from LMs is risky, as they often contain incorrect or misleading information. Why is this happening? We identify two key issues: (1) ambiguous and underspecified user questions and (2) imperfect knowledge in LMs, especially for long tail or recent events. To address the first issue, we propose a system that can interact with users to clarify their intent before answering. By simulating their expected outcomes in the future turns, we reward LMs for generating clarifying questions and not just answering immediately. In the second part of the talk, I will discuss the state of retrieval augmentation, which is often lauded as the path to provide up-to-date, relevant knowledge to LMs. While their success is evident in scenarios where there exists a single gold document, incorporating information from a diverse set of documents remains challenging for both retrievers and LMs. Together, the talk highlights key research directions for building reliable LMs to answer information seeking questions. <br/><br/>
                    <b>Bio:</b> Eunsol Choi is an assistant professor of computer science and data science at New York University. Her research spans natural language processing and machine learning, with a focus on interpreting and reasoning about text in dynamic real-world contexts. Prior to joining NYU, she was an assistant professor at the University of Texas at Austin. She also spent a year at Google AI as a visiting researcher. She holds a Ph.D. in computer science and engineering from the University of Washington. She is a recipient of a Facebook research fellowship, Google faculty research award, Sony faculty award, and an outstanding paper award at EMNLP. <br/><br/><br/>

                    <h5 id="Dawei">Dawei Huang</h5>
                    <b>Title:</b> Enabling extremely fast inference and training performance using dataflow and custom chip<br/><br/>
                    <b>Abstract:</b> As the pursuit of larger language models continues to push the boundaries of computational demands, the traditional silicon chip is facing a daunting memory/power wall. In this talk, we present a novel chip design, SN40L, to tackle this challenge. This chip combines a reconfigurable data-flow architecture with a tightly coupled 3-tier memory hierarchy to enable efficient compute-intensive training and memory-bound inference workloads for a wide variety of neural network architectures. We discuss the various advantages of this chip via case studies. In our first case study, we discuss how the dataflow architecture coupled with on-chip SRAM and HBM empowers operation fusion capabilities enabling 1000+ tokens/second inference performance without sacrificing on precision. In the second case study we look at the training performance of various model architectures and compare their performance against traditional kernel by kernel execution-based architectures. We show how dataflow architecture can help accelerate LLM training for traditional dense, sparse and novel state space models, while allowing one to train extremely large models on a smaller footprint. In the third case study we will discuss how you can use the strongly coupled DRAM, HBM and SRAM to develop new neural network architecture that can help scale to 100+ billion parameters efficiently. This new architecture uses a modular and coarse-grained approach to a mixture of experts that allows for incremental updates to models for new capabilities and knowledge and smaller footprint execution during inference. <br/><br/>
                    <b>Bio:</b> Dawei Huang received BS from Tsinghua University and MS from UCSD. He has 25+ years experience in Computer/AI Systems and has 47 issued patents. Currently, he is Senior Director of Engineering at Sambanova Systems. His current responsibilities include ML Model Performance and Customer Solution. <br/><br/><br/>

                    <h5 id="Taylor">Taylor Berg-Kirkpatrick</h5>
                    <b>Title:</b> Large Language Models and Cybersecurity: New Threats and Safeguards<br/><br/>
                    <b>Abstract:</b> The rapid development of large-language models has created new attack surfaces for adversaries, but at the same time has enabled new mitigations for human-driven fraud and social engineering attacks. This talk will consider two new interfaces between NLP and computer security: (1) the potential for new types of attacks as LLMs are increasingly treated like operating systems and (2) the possibility of using LLMs to combat and track social engineering attacks at scale via LLM-driven honeypots. <br/><br/>
                    <b>Bio:</b> Taylor Berg-Kirkpatrick is an Associate Professor in the Computer Science and Engineering Department at the University of California San Diego. Taylor's research focuses on using machine learning to understand structured human data, including language but also sources like music, document images, and other complex artifacts. <br/><br/><br/>
        </div>

        <br/>


        <div id="paper" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Accepted Work</h4>
                    <p>Non-archival, randomly ordered</p>
                    <hr>
                </div>
            </div>

            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h5> Poster Session #1 (11:30am - 12:30pm)</h5>
                    <p><ol start="1">

<li><b><font class="uci-blue">CLIMB: A Benchmark of Clinical Bias in Large Language Models</font></b> <br>Yubo Zhang, Shudi Hou, Mingyu Derek Ma, Wei Wang, Muhao Chen, Jieyu Zhao</li>
<li><b><font class="uci-blue">Exploring the Design Space of Diffusion Bridge Models via Stochasticity Control</font></b> <br>Shaorong Zhang, Yuanbin Cheng, Xianghao Kong, Greg Ver Steeg</li>
<li><b><font class="uci-blue">A Novel Multi-Document Retrieval Benchmark Grounded on Journalist Source-Selection in Newswriting</font></b> <br>Alexander Spangher, Tenghao Huang, Yiqin Huang, Liheng Lai, Lucas Spangher, Sewon Min, Mark Dredze</li>
<li><b><font class="uci-blue">Chain-of-Thought Augmentation with Logit Contrast for Enhanced Reasoning in Language Models</font></b> <br>Jay Shim, Grant Kruttschnitt, Alyssa Ma, Daniel Kim, Benjamin Chek, Athul Anand, Kevin Zhu, Sean O'Brien</li>
<li><b><font class="uci-blue">Improving Inference on Theory of Mind with Evidence Chain</font></b> <br>Qiutong Tony Yi, Wang Zhu</li>
<li><b><font class="uci-blue">An Audio checkup for Language Models</font></b> <br>Arjun Prasaath Anbazhagan, Parteek Kumar, Souritra Kar, Ujjwal Kaur</li>
<li><b><font class="uci-blue">REFFLY: Melody-Constrained Lyrics Editing Model</font></b> <br>Songyan Zhao, Bingxuan Li, Yufei Tian, Nanyun Peng</li>
<li><b><font class="uci-blue">Compare without Despair: Reliable Preference Evaluation with Generation Separability</font></b> <br>Sayan Ghosh, Tejas Srinivasan, Swabha Swayamdipta</li>
<li><b><font class="uci-blue">Towards Interventions for Suicide Prevention with LLM Assistants in Social Work</font></b> <br>Jaspreet Ranjit, Hyundong Justin Cho, Myles Phung, John R. Blosnich, Swabha Swayamdipta</li>
<li><b><font class="uci-blue">Verify with Caution: The Pitfalls of Relying on Imperfect Factuality Metrics</font></b> <br>Ameya Godbole, Robin Jia</li>
<li><b><font class="uci-blue">The Dark Side of Instagram: A Large Dataset for Identifying Persian Harmful Comments</font></b> <br>Hadi Davardoust, Hadi Zare, Hossein Rafiee zade</li>
<li><b><font class="uci-blue">Eliciting Better Multilingual Structured Reasoning from LLMs through Code</font></b> <br>Bryan Li, Tamer Alkhouli, Daniele Bonadiman, Nikolaos Pappas, Saab Mansour</li>
<li><b><font class="uci-blue">Unlocking Decoding-Time Controllability: Gradient-Free Multi-Objective Alignment with Contrastive Prompts</font></b> <br>Tingchen Fu, Yupeng Hou, Julian McAuley, Rui Yan</li>
<li><b><font class="uci-blue">AgentGrow: LLMs as Scalable, Customizable General-Purpose Simulators For Language Agent Training</font></b> <br>Yiming Wang, Yuedong Cui, Da Yin, Zongyu Lin, Di Wu, Xueqing Wu, Chenchen Ye, Kai-Wei Chang</li>
<li><b><font class="uci-blue">SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement</font></b> <br>Antonis Antoniades, Albert Örwall, Kexun Zhang, Yuxi Xie, Anirudh Goyal, William Yang Wang</li>
<li><b><font class="uci-blue">INTERINTENT: Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context</font></b> <br>Ziyi Liu, Abhishek Anand, Pei Zhou, Jen-tse Huang, Jieyu Zhao</li>
<li><b><font class="uci-blue">Contrastive Instruction Tuning</font></b> <br>Tianyi Lorena Yan, Fei Wang, James Y. Huang, Wenxuan Zhou, Fan Yin, Aram Galstyan, Wenpeng Yin, Muhao Chen</li>
<li><b><font class="uci-blue">Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</font></b> <br>Manling Li, Shiyu Zhao, Qineng Wang, Kangrui Wang, Yu Zhou, Sanjana Srivastava, Cem Gokmen, Tony Lee, Li Erran Li, Ruohan Zhang, Weiyu Liu, Percy Liang, Li Fei-Fei, Jiayuan Mao, Jiajun Wu</li>
<li><b><font class="uci-blue">Teaching Models to Understand but Not Generate</font></b> <br>Ryan Yixiang Wang, Matthew Finlayson, Luca Soldaini, Robin Jia, Swabha Swayamdipta</li>
<li><b><font class="uci-blue">IsoBench: Benchmarking Multimodal Foundation Models on Isomorphic Representations</font></b> <br>Deqing Fu, Ruohao Guo, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, Willie Neiswanger</li>
<li><b><font class="uci-blue">IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models</font></b> <br>Haz Sameen Shahgir, Khondker Salman Sayeed, Abhik Bhattacharjee, Wasi Uddin Ahmad, Yue Dong, Rifat Shahriyar</li>
<li><b><font class="uci-blue">Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack</font></b> <br>Xiaoyue Xu, Qinyuan Ye, Xiang Ren</li>
<li><b><font class="uci-blue">Can Textual Unlearning Solve Cross-Modality Safety Alignment?</font></b> <br>Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael Abu-Ghazaleh, M. Salman Asif, Yue Dong, Amit Roy-Chowdhury, Chengyu Song</li>
<li><b><font class="uci-blue">VDebugger: Harnessing Execution Feedback for Debugging Visual Programs</font></b> <br>Xueqing Wu, Zongyu Lin, Songyan Zhao, Te-Lin Wu, Pan Lu, Nanyun Peng, Kai-Wei Chang</li>
<li><b><font class="uci-blue">Evaluating Grounding Gaps in LLMs via News Interviews</font></b> <br>Michael Lu, Hyundong Justin Cho, Weiyan Shi, Jonathan May, Alexander Spangher</li>
<li><b><font class="uci-blue">LogogramNLP: Comparing Visual and Textual Representations of Ancient Logographic Writing Systems for NLP</font></b> <br>Danlu Chen, Freda Shi, Aditi Agarwal, Jacobo Myerston, Taylor Berg-Kirkpatrick</li>
<li><b><font class="uci-blue">Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks</font></b> <br>Dharunish Yugeswardeenoo, Kevin Zhu, Sean O'Brien</li>
<li><b><font class="uci-blue">AgentReview: Exploring Peer Review Dynamics with LLM Agents</font></b> <br>Yiqiao Jin, Qinlin Zhao, Yiyang Wang, Hao Chen, Kaijie Zhu, Yijia Xiao, Jindong Wang</li>
<li><b><font class="uci-blue">Answer is All You Need: Instruction-following Text Embedding via Answering the Question</font></b> <br>Letian Peng, Yuwei Zhang, Zilong Wang, Jayanth Srinivasa, Gaowen Liu, Zihan Wang, Jingbo Shang</li>
<li><b><font class="uci-blue">Socratic Mind: Scalable Oral Assessment Powered By AI</font></b> <br>Jui-Tse Hung, Christopher Zhang Cui, Diana M. Popescu, Saurabh Chatterjee, Thad Starner</li>
<li><b><font class="uci-blue">Will Trump Win in 2024? Predicting the US Presidential Election via Multi-step Reasoning with Large Language Models</font></b> <br>Chenxiao Yu, Zhaotian Weng, Xiyang Hu, Yue Zhao</li>
<li><b><font class="uci-blue">Faithful Persona-based Conversational Dataset Generation with Large Language Models</font></b> <br>Pegah Jandaghi, Xianghai Sheng, Xinyi Bai, Jay Pujara, Hakim Sidahmed</li>
<li><b><font class="uci-blue">Exploring Scientific Hypothesis Generation with Mamba</font></b> <br>Miaosen Chai, Emily Herron, Erick Cervantes, Tirthankar Ghosal</li>
<li><b><font class="uci-blue">From Bias to Balance: Detecting Facial Expression Recognition Biases in Multimodal Foundation Models</font></b> <br>Kaylee Chhua, Zhoujinyi Wen, Vedant Hathalia, Kevin Zhu, Sean O'Brien</li>
<li><b><font class="uci-blue">AAVENUE: Detecting LLM Biases on NLU Tasks in AAVE via a Novel Benchmark</font></b> <br>Abhay Gupta, Ece Yurtseven, Philip Meng, Sean O'Brien, Kevin Zhu</li>
<li><b><font class="uci-blue">Language Models Implicitly Learn a Unified Representation Space</font></b> <br>Zhaofeng Wu, Xinyan Velocity Yu, Dani Yogatama, Jiasen Lu, Yoon Kim</li>
<li><b><font class="uci-blue">Scaling LLM Inference with Optimized Sample Compute Allocation</font></b> <br>Kexun Zhang, Shang Zhou, Danqing Wang, William Yang Wang, Lei Li</li>
<li><b><font class="uci-blue">Evaluating Human Alignment and Model Faithfulness of LLM Rationale</font></b> <br>Mohsen Fayyaz, Fan Yin, Jiao Sun, Nanyun Peng</li>
<li><b><font class="uci-blue">Guiding Through Complexity: What Makes Good Supervision for Hard Reasoning Tasks?</font></b> <br>Xuan He, Da Yin, Nanyun Peng</li>
<li><b><font class="uci-blue">A Little Human Data Goes A Long Way</font></b> <br>Dhananjay Ashok, Jonathan May</li>
<li><b><font class="uci-blue">Enhancing Depression Diagnosis with Chain-of-Thought Prompting</font></b> <br>Elysia Shi, Adithri Manda, London Chowdhury, Runeema Arun, Kevin Zhu, Michael Lam</li>
<li><b><font class="uci-blue">ChunkRAG: Novel LLM-Chunk Filtering Method for RAG Systems</font></b> <br>Ishneet Sukhvinder Singh, Ritvik Aggarwal, Ibrahim Allahverdiyev, Muhammad Taha, Aslihan Akalin, Kevin Zhu, Sean O'Brien</li>
<li><b><font class="uci-blue">Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference</font></b> <br>Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana Rao Kompella, Sijia Liu, Shiyu Chang</li>
<li><b><font class="uci-blue">DeLLMa: Decision Making Under Uncertainty with Large Language Models</font></b> <br>Ollie Liu, Deqing Fu, Dani Yogatama, Willie Neiswanger</li>
<li><b><font class="uci-blue">Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans</font></b> <br>Jen-tse Huang, Man Ho LAM, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael Lyu</li>
<li><b><font class="uci-blue">Interleaved Multimodal Decision Transformer for Embodied Agents</font></b> <br>Bosung Kim</li>
<li><b><font class="uci-blue">DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks</font></b> <br>Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie</li>
<li><b><font class="uci-blue">Learn from Failure: Fine-Tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving</font></b> <br>Chenyang An, Zhibo Chen, Qihao Ye, Emily First, Letian Peng, Jiayun Zhang, Zihan Wang, Sorin Lerner, Jingbo Shang</li>
<li><b><font class="uci-blue">Control Large Language Models via Divide and Conquer</font></b> <br>Bingxuan Li, Yiwei Wang, Tao Meng, Kai-Wei Chang, Nanyun Peng</li>
<li><b><font class="uci-blue">On the Resilience of Multi-Agent Systems with Malicious Agents</font></b> <br>Jen-tse Huang, Jiaxu Zhou, Tailin Jin, Xuhui Zhou, Zixi Chen, Wenxuan Wang, Youliang Yuan, Maarten Sap, Michael Lyu</li>
<li><b><font class="uci-blue">Simplicity Bias of Transformers to Learn Low Sensitivity Functions</font></b> <br>Bhavya Vasudeva, Deqing Fu, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan</li>
<li><b><font class="uci-blue">ClimaQA: An Automated Evaluation Framework for Climate Foundation Models</font></b> <br>Veeramakali Vignesh Manivannan, Yasaman Jafari, Srikar Eranky, Spencer Ho, Rose Yu, Duncan Watson-Parris, Yian Ma, Leon Bergen, Taylor Berg-Kirkpatrick</li>
<li><b><font class="uci-blue">Q-Debias: Quantum-Enhanced ActAdd-Guided Bias Reduction in LLMs</font></b> <br>Shardul Kulkarni, Rishi Gandhe, Ryunosuke Tatlock, Dylan De Schryver, Daniel Cho, Jonathan Lu, Kevin Zhu</li>
<li><b><font class="uci-blue">Images Speak Louder than Words: Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective</font></b> <br>Zhaotian Weng, Zijun Gao, Jerone Andrews, Jieyu Zhao</li>
<li><b><font class="uci-blue">Explain it to me like I’m five: Are LLM Explanations Effective for All Users?</font></b> <br>Brihi Joshi, Swabha Swayamdipta, Xiang Ren</li>

                    </ol></p>

                    <h5> Poster Session #2 (1:30pm - 2:30pm)</h5>
                    <p><ol start="56">

<li><b><font class="uci-blue">MultiAgent Collaboration Attack: Investigating Adversarial Attacks in Large Language Model Collaborations via Debate</font></b> <br>Alfonso Amayuelas</li>
<li><b><font class="uci-blue">The African Languages Lab: Advancing NLP for Underrepresented African Languages through Global Collaboration</font></b> <br>Sheriff Issaka</li>
<li><b><font class="uci-blue">Autonomous agents from automatic reward modeling and planning</font></b> <br>Rui Sun</li>
<li><b><font class="uci-blue">Euclid: Supercharging Multimodal LLMs with Synthetic High-Fidelity Visual Descriptions</font></b> <br>Jiarui Zhang, Ollie Liu, Tianyu Yu, Jinyi Hu, Willie Neiswanger</li>
<li><b><font class="uci-blue">Clustering Running Titles to Understand the Printing of Early Modern Books</font></b> <br>Nikolai Vogler, Kartik Goyal, Christopher Warren, Max G'Sell, Taylor Berg-Kirkpatrick</li>
<li><b><font class="uci-blue">Evaluating K-Fold Cross Validation for Transformer Based Symbolic Regression Models</font></b> <br>Kaustubh Kislay, Shlok Singh, Rohan Dutta, Soham Joshi, Jay Shim, George Flint, Kevin Zhu</li>
<li><b><font class="uci-blue">Optimizing LLMs with K-Shot Prompting for Synthetic Data Generation</font></b> <br>Vivaan Sehgal, Prahalad Anand, Brian Lin, Shridhar Garg, George Flint, Kevin Zhu</li>
<li><b><font class="uci-blue">Semantic Self-Consistency: Enhancing Language Model Reasoning via Semantic Weighting</font></b> <br>Tim Knappe, Ryan Luo Li, Ayush Chauhan, Kaylee Chhua, Kevin Zhu, Sean O'Brien</li>
<li><b><font class="uci-blue">Q*Agent: Optimizing Language Agents with Q-Guided Exploration</font></b> <br>Zongyu Lin, Yao Tang, Da Yin, Xingcheng Yao, Ziniu Hu, Yizhou Sun, Kai-Wei Chang</li>
<li><b><font class="uci-blue">3D-LLM: Injecting the 3D World into Large Language Models</font></b> <br>Yining Hong</li>
<li><b><font class="uci-blue">ContextRef: Evaluating referenceless metrics for image description generation</font></b> <br>Elisa Kreiss, Eric Zelikman, Christopher Potts, Nick Haber</li>
<li><b><font class="uci-blue">Detecting Machine-Generated Long-Form Content with Latent-Space Variables</font></b> <br>Yufei Tian, Zeyu Pan, Nanyun Peng</li>
<li><b><font class="uci-blue">A Debate-Driven Experiment on LLM Hallucinations and Accuracy</font></b> <br>Ruikun Li, Tanishka Bagade, Kevin Martinez, Flora Yasmin, Grant Ayala, Michael Lam, Kevin Zhu</li>
<li><b><font class="uci-blue">Can Large Language Models Successfully Understand Errors?</font></b> <br>Jason Li, Lauren Gabrielle Yraola, Kevin Zhu, Sean O'Brien</li>
<li><b><font class="uci-blue">Promoting Fairness in Link Prediction with Graph Enhancement</font></b> <br>Yezi Liu, Hanning Chen, Mohsen Imani</li>
<li><b><font class="uci-blue">Stronger Random Baselines for In-Context Learning</font></b> <br>Gregory Yauney, David Mimno</li>
<li><b><font class="uci-blue">Challenges in Cross-Lingual Transfer Learning for  Offensive Language Detection: Insights from 30 Languages</font></b> <br>Elnaz Rahmati, Alireza Salkhordeh Ziabari, Morteza Dehghani</li>
<li><b><font class="uci-blue">Beyond LLMs: A Linguistic Approach to Causal Graph Generation from Narrative Texts</font></b> <br>Zehan Li, Ruhua Pan, Xinyu Pi</li>
<li><b><font class="uci-blue">Attributing Culture-Conditioned Generations to Pretraining Corpora</font></b> <br>Huihan Li, Arnav Goel, Keyu He, Xiang Ren</li>
<li><b><font class="uci-blue">BPO: Staying Close to the Behavior LLM Creates Better Online LLM Alignment</font></b> <br>Wenda Xu, Jiachen Li, William Yang Wang, Lei Li</li>
<li><b><font class="uci-blue">Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack</font></b> <br>Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong</li>
<li><b><font class="uci-blue">Are Large-Language Models Graph Algorithmic Reasoners?</font></b> <br>Alexander K Taylor, Anthony Cuturrufo, Vishal Yathish, Mingyu Derek Ma, Wei Wang</li>
<li><b><font class="uci-blue">Can Language Models Learn to Skip Steps?</font></b> <br>Tengxiao Liu, Qipeng Guo, Xiangkun Hu, Cheng Jiayang, Yue Zhang, Xipeng Qiu, Zheng Zhang</li>
<li><b><font class="uci-blue">Open-world Multi-label Text Classification with Extremely Weak Supervision</font></b> <br>Xintong Li, Jinya Jiang, Ria Dharmani, Jayanth Srinivasa, Gaowen Liu, Jingbo Shang</li>
<li><b><font class="uci-blue">The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention</font></b> <br>Yixin Wan, Di Wu, Haoran Wang, Kai-Wei Chang</li>
<li><b><font class="uci-blue">Enhancing Transparency in RLHF: An Explainable Reward Framework for Aligning Large Language Models</font></b> <br>Yiran Shen, Aditya Emmanuel Arokiaraj John, Brandon Fain</li>
<li><b><font class="uci-blue">Dialect Bias in Affective Computing: African American English and Text-Based Emotion Classification</font></b> <br>Rebecca Dorn, Christina A Chance, Casandra Rusti</li>
<li><b><font class="uci-blue">Cross-Task Generalization Abilities of Large Language Models</font></b> <br>Qinyuan Ye</li>
<li><b><font class="uci-blue">Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling</font></b> <br>Hritik Bansal, Arian Hosseini, Rishabh Agarwal, Vinh Q. Tran, Mehran Kazemi</li>
<li><b><font class="uci-blue">Machine Learning Research Transparency through Positionality Statements</font></b> <br>Christina A Chance, Rebecca Pattichis</li>
<li><b><font class="uci-blue">Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization</font></b> <br>Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover</li>
<li><b><font class="uci-blue">Matryoshka Query Transformer for Large Vision-Language Models</font></b> <br>Wenbo Hu, Zi-Yi Dou, Liunian Harold Li, Amita Kamath, Nanyun Peng, Kai-Wei Chang</li>
<li><b><font class="uci-blue">Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models</font></b> <br>Somanshu Singla, Zhen Wang, Tianyang Liu, Abdullah Ashfaq, Zhiting Hu, Eric P. Xing</li>
<li><b><font class="uci-blue">Sufficient Context: A New Lens on Retrieval Augmented Generation Systems</font></b> <br>Hailey Joren, Jianyi Zhang, Chun-Sung Ferng, Da-Cheng Juan, Ankur Taly, Cyrus Rashtchian</li>
<li><b><font class="uci-blue">LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models</font></b> <br>Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang</li>
<li><b><font class="uci-blue">Speechworthy Instruction-tuned Language Models</font></b> <br>Hyundong Justin Cho, Nicolaas Paul Jedema, Leonardo F. R. Ribeiro, Karishma Sharma, Pedro Szekely, Alessandro Moschitti, Ruben Janssen, Jonathan May</li>
<li><b><font class="uci-blue">EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing</font></b> <br>Kaizhi Zheng, Xiaotong Chen, Xuehai He, Jing Gu, Linjie Li, Zhengyuan Yang, Kevin Lin, Jianfeng Wang, Lijuan Wang, Xin Eric Wang</li>
<li><b><font class="uci-blue">Using Linguistic Entrainment to Evaluate Large Language Models for use in Cognitive Behavioral Therapy</font></b> <br>Mina Kian, Kaleen Shrestha, Katrin Fischer, Xiaoyuan Zhu, Jonathan Ong, Aryan Trehan, Jessica Wang, Gloria Chang, Séb Arnold, Maja Mataric</li>
<li><b><font class="uci-blue">VSP: Assessing the dual challenges of perception and reasoning in spatial planning tasks for MLLMs</font></b> <br>Qiucheng Wu</li>
<li><b><font class="uci-blue">Quantifying Generalization Complexity for Large Language Models</font></b> <br>Zhenting Qi, Hongyin Luo, Xuliang Huang, Zhuokai Zhao, Yibo Jiang, Xiangjun Fan, Himabindu Lakkaraju, James R. Glass</li>
<li><b><font class="uci-blue">LlamaTales: Studying the Effects of Training Data on Small Language Models</font></b> <br>Ivan Lee, Taylor Berg-Kirkpatrick</li>
<li><b><font class="uci-blue">A Comparative Study of Translation Bias and Accuracy in Multilingual Large Language Models for Cross-Language Claim Verification</font></b> <br>Aryan Singhal, Veronica Shao, Gary Sun, Ryan Ding, Jonathan Lu, Kevin Zhu</li>
<li><b><font class="uci-blue">Town Hall Debate Prompting: Enhancing Logical Reasoning in LLMs through Multi-Persona Interaction</font></b> <br>Bhav Jain, Vivaan Sandwar, Ishaan Garg, Rishan Thangaraj, Michael Lam, Kevin Zhu</li>
<li><b><font class="uci-blue">StyleRemix: Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements</font></b> <br>Jillian Fisher, Skyler Hallinan, Ximing Lu, Mitchell L Gordon, Zaid Harchaoui, Yejin Choi</li>
<li><b><font class="uci-blue">Goal-Driven Explainable Clustering via Language Descriptions</font></b> <br>Zihan Wang, Jingbo Shang, Ruiqi Zhong</li>
<li><b><font class="uci-blue">Inferring from Logits: Exploring Best Practices for Decoding-Free Generative Candidate Selection</font></b> <br>Mingyu Derek Ma, Yanna Ding, Zijie Huang, Jianxi Gao, Yizhou Sun, Wei Wang</li>
<li><b><font class="uci-blue">Follow My Instruction and Spill the Beans: Scalable Data Extraction from Retrieval-Augmented Generation Systems</font></b> <br>Zhenting Qi, Hanlin Zhang, Eric P. Xing, Sham M. Kakade, Himabindu Lakkaraju</li>
<li><b><font class="uci-blue">Generative Verifiers: Reward Modeling as Next-Token Prediction</font></b> <br>Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, Rishabh Agarwal</li>
<li><b><font class="uci-blue">Leveraging Language Models to Detect Greenwashing</font></b> <br>Margaret Capetz, Christina A Chance, Rebecca Pattichis, Reshmi Ghosh, Avalon Vinella</li>
<li><b><font class="uci-blue">Asymmetric Bias in Text-to-Image Generation with Adversarial Attacks</font></b> <br>Haz Sameen Shahgir, Xianghao Kong, Greg Ver Steeg, Yue Dong</li>
<li><b><font class="uci-blue">Enhancing Knowledge Distillation for LLMs with Response-Priming Prompting</font></b> <br>Vijay Goyal, Mustafa Khan, Aprameya Tirupati, Harveer Saini, Michael Lam, Kevin Zhu</li>
<li><b><font class="uci-blue">Learning a Decision Tree Algorithm with Transformers</font></b> <br>Yufan Zhuang, Liyuan Liu, Chandan Singh, Jingbo Shang, Jianfeng Gao</li>
<li><b><font class="uci-blue">Evaluating Creativity and Deception in Large Language Models: A Simulation Framework for Multi-Agent Balderdash</font></b> <br>Parsa Hejabi, Elnaz Rahmati, Alireza Salkhordeh Ziabari, Preni Golazizian, Jesse Thomason, Morteza Dehghani</li>
<li><b><font class="uci-blue">Planning as Inpainting: A Generative Framework for Realistic Embodied Path Planning</font></b> <br>Cheng-Fu Yang, Kai-Wei Chang</li>
<li><b><font class="uci-blue">Multi-step Problem Solving Through a Verifier: An Empirical Analysis on Model-induced Process Supervision</font></b> <br>Zihan Wang, Yunxuan Li, Yuexin Wu, Liangchen Luo, Le Hou, Hongkun Yu, Jingbo Shang</li>

                    </ol></p>

                    <h5> Poster Session #3 (2:40pm - 3:40pm)</h5>
                    <p><ol start="111">
<li><b><font class="uci-blue">Pre-trained Large Language Models Use Fourier Features to Compute Addition</font></b> <br>Tianyi Zhou, Deqing Fu, Vatsal Sharan, Robin Jia</li>
<li><b><font class="uci-blue">NusaMT-7B: Machine Translation for Low-Resource Indonesian Languages with Large Language Models</font></b> <br>William Tan, Kevin Zhu</li>
<li><b><font class="uci-blue">Adaptive In-conversation Team Building for Language Model Agents</font></b> <br>Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang</li>
<li><b><font class="uci-blue">PatentEdits: Framing Patent Novelty as Textual Entailment</font></b> <br>Ryan Lee, Alexander Spangher, Xuezhe Ma</li>
<li><b><font class="uci-blue">Model Editing Harms General Abilities of Large Language Models: Regularization to the Rescue</font></b> <br>Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, Nanyun Peng</li>
<li><b><font class="uci-blue">Fictitious Synthetic Data Can Improve LLM Factuality via Prerequisite Learning</font></b> <br>Yujian Liu</li>
<li><b><font class="uci-blue">Adversarial Attacks on Parts of Speech: An Empirical Study in Text-to-Image Generation</font></b> <br>G M Shahariar, Jia Chen, Jiachen Li, Yue Dong</li>
<li><b><font class="uci-blue">SoMeR: A Multi-View Social Media User Representation Learning Framework</font></b> <br>Keith Burghardt, Siyi Guo, Valeria Pantè, Kristina Lerman</li>
<li><b><font class="uci-blue">Communicate to Play: Pragmatic Reasoning for Efficient Cross-Cultural Communication</font></b> <br>Isadora White, Sashrika Pandey, Michelle Pan</li>
<li><b><font class="uci-blue">Weak-to-Strong Reasoning</font></b> <br>Yuqing Yang, Yan Ma, Pengfei Liu</li>
<li><b><font class="uci-blue">MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning</font></b> <br>Yifan Jiang, Jiarui Zhang, Kexuan Sun, Zhivar Sourati, Kian Ahrabian, Kaixin Ma, Filip Ilievski, Jay Pujara</li>
<li><b><font class="uci-blue">STAR: A Simple Training-free Approach for Recommendations using Large Language Models</font></b> <br>Dong-Ho Lee, Adam Kraft, Long Jin, Nikhil Mehta, Taibai Xu, Lichan Hong, Ed H. Chi, Xinyang Yi</li>
<li><b><font class="uci-blue">Ladder Residual: Redefining Tensor Parallelism in Transformers for Accelerated Inference</font></b> <br>Muru Zhang, Mayank Mishra, Zhongzhu Zhou, William Brandon, Jue WANG, Yoon Kim, Jonathan Ragan-Kelley, Shuaiwen Leon Song, Ben Athiwaratkun, Tri Dao</li>
<li><b><font class="uci-blue">CLEAR: Contrastive Learning with Experts and Amateurs for Reasoning</font></b> <br>Andrew Rufail, Abenezer Tessema, Buse Toksoz, Chizoba Okoli, Jonathan Pei, Kevin Zhu</li>
<li><b><font class="uci-blue">Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models</font></b> <br>Lucas Bandarkar</li>
<li><b><font class="uci-blue">MORL-Prompt: An Empirical Analysis of Multi-Objective Reinforcement Learning for Discrete Prompt Optimization</font></b> <br>Yasaman Jafari, Dheeraj Mekala, Rose Yu, Taylor Berg-Kirkpatrick</li>
<li><b><font class="uci-blue">Evaluating Cultural and Social Awareness of LLM Web Agents</font></b> <br>Haoyi Qiu, Alexander Fabbri, Divyansh Agarwal, Kung-Hsiang Huang, Sarah Tan, Nanyun Peng, Chien-Sheng Wu</li>
<li><b><font class="uci-blue">VideoPhy: Evaluating Physical Commonsense for Video Generation</font></b> <br>Hritik Bansal, Zongyu Lin, Tianyi Xie, Zeshun Zong, Michal Yarom, Yonatan Bitton, Chenfanfu Jiang, Yizhou Sun, Kai-Wei Chang, Aditya Grover</li>
<li><b><font class="uci-blue">COMMUNITY-CROSS-INSTRUCT: Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities</font></b> <br>Zihao He, Minh Duc Chu, Rebecca Dorn, Siyi Guo, Kristina Lerman</li>
<li><b><font class="uci-blue">Exploring Synthetic Datasets for Large Language Model Unlearning</font></b> <br>Xiaoyuan Zhu, Ollie Liu, Muru Zhang, Willie Neiswanger</li>
<li><b><font class="uci-blue">MAPO: Momentum-Aided Gradient Descent Prompt Optimization</font></b> <br>Anthony Cui, Pranav Nandyalam, Kevin Zhu</li>
<li><b><font class="uci-blue">Out-of-Distribution Detection through Soft Clustering with Non-Negative Kernel Regression</font></b> <br>Aryan Gulati, Xingjian Dong, Carlos Hurtado, Sarath Shekkizhar, Swabha Swayamdipta, Antonio Ortega</li>
<li><b><font class="uci-blue">CoLLAP: Contrastive Long-form Language-Audio Pretraining with Musical Temporal Structure Augmentation</font></b> <br>Junda Wu, Warren Li, Zachary Novack, Amit Namburi, Carol Chen, Julian McAuley</li>
<li><b><font class="uci-blue">Data Contamination Can Cross Language Barriers</font></b> <br>Feng Yao, Yufan Zhuang, Zihao Sun, Sunan Xu, Animesh Kumar, Jingbo Shang</li>
<li><b><font class="uci-blue">Quantifying and Optimizing Global Faithfulness in Persona-driven Role-playing</font></b> <br>Letian Peng, Jingbo Shang</li>
<li><b><font class="uci-blue">DiversityMedQA: A Benchmark for Assessing Demographic Biases in Medical Diagnosis using Large Language Models</font></b> <br>Rajat Rawat, Hudson McBride, Rajarshi Ghosh, Dhiyaan Chakkresh Nirmal, Jong Moon, Dhruv Karthik Alamuri, Sean O'Brien, Kevin Zhu</li>


                    
<li><b><font class="uci-blue">Language Models Can Infer Action Semantics for Symbolic Planners from Environment Feedback</font></b> <br>Wang Zhu, Ishika Singh, Robin Jia, Jesse Thomason</li>
<li><b><font class="uci-blue">A Probabilistic Framework for LLM Hallucination Detection via Belief Tree Propagation</font></b> <br>Bairu Hou, Shiyu Chang</li>
<li><b><font class="uci-blue">GPT is Not an Annotator: The Necessity of Human Annotation in Fairness Benchmark Construction</font></b> <br>Virginia K. Felkner, Jennifer A. Thompson, Jonathan May</li>
<li><b><font class="uci-blue">Adaptable Logical Control for Large Language Models</font></b> <br>Honghua Zhang, Po-Nien Kung, Masahiro Yoshida, Guy Van den Broeck, Nanyun Peng</li>
<li><b><font class="uci-blue">Strategic Moves: Negotiation Agents with Dynamic Adaptation to Opponent Behavior for Optimizing Counter-Offers</font></b> <br>Deuksin Kwon, Jiwon Hae, Gale Lucas</li>
<li><b><font class="uci-blue">Solving for X and Beyond: Can Large Language Models Solve Complex Math Problems with More-Than-Two Unknowns?</font></b> <br>Kuei-Chun Kao, Ruochen Wang, Cho-Jui Hsieh</li>
<li><b><font class="uci-blue">Capturing the Essence of a Phrase: Extracting Physical and Sensory Information from Text</font></b> <br>Abhinav Gupta</li>
<li><b><font class="uci-blue">Efficient LLM Scheduling by Learning to Rank</font></b> <br>Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, Hao Zhang</li>
<li><b><font class="uci-blue">Improving LLM Abilities in Idiomatic Translation</font></b> <br>Maximilian Spencer, Sundesh Donthi, Om Patel, Joon Young Doh, Eid Rodan</li>
<li><b><font class="uci-blue">Large Language Models Can Be Contextual Privacy Protection Learners</font></b> <br>Yijia Xiao, Yiqiao Jin, Yushi Bai, Yue Wu, Xianjun Yang, Xiao Luo, Wenchao Yu, Xujiang Zhao, Yanchi Liu, Quanquan Gu, Haifeng Chen, Wei Wang, Wei Cheng</li>
<li><b><font class="uci-blue">TrimLLM: Progressive Layer Dropping for Efficient LLM Fine-tuning and Inference</font></b> <br>Lanxiang Hu, Tajana Rosing, Hao Zhang</li>
<li><b><font class="uci-blue">Revisiting Representation Collapse in Language Models</font></b> <br>Atharva Kulkarni, Swabha Swayamdipta</li>
<li><b><font class="uci-blue">Logits of API-Protected LLMs Leak Proprietary Information</font></b> <br>Matthew Finlayson, Swabha Swayamdipta, Xiang Ren</li>
<li><b><font class="uci-blue">Grammar-Aligned Decoding</font></b> <br>Kanghee Park, Jiayu Wang, Taylor Berg-Kirkpatrick, Nadia Polikarpova, Loris D'Antoni</li>
<li><b><font class="uci-blue">Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation</font></b> <br>Di Wu, Jia-Chen Gu, Fan Yin, Nanyun Peng, Kai-Wei Chang</li>
<li><b><font class="uci-blue">Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore</font></b> <br>Michael Saxon, Fatima Jahara, Mahsa Khoshnoodi, Yujie Lu, Aditya Sharma, William Yang Wang</li>
<li><b><font class="uci-blue">LLM Self-contradictory Hallucination Mitigation via Retrieval-Based Post-Processing</font></b> <br>Zhaotian Weng, Xingjian Dong, Liyan Huang, Jingwen Qi</li>
<li><b><font class="uci-blue">When Parts Are Greater Than Sums: Individual LLM Components Can Outperform Full Models</font></b> <br>Ting-Yun Chang, Jesse Thomason, Robin Jia</li>
<li><b><font class="uci-blue">Evaluating LLM Chatbot-Generated Answers to Patient Questions</font></b> <br>Wang Zhu, Tian-qi Chen, Ruishan Liu, Robin Jia</li>
<li><b><font class="uci-blue">The Strong Pull of Prior Knowledge in Large Language Models and Its Impact on Emotion Recognition</font></b> <br>Georgios Chochlakis, Alexandros Potamianos, Kristina Lerman, Shrikanth Narayanan</li>
<li><b><font class="uci-blue">Firm’s AI Exposure without Labor Data: The Expected Impact of AI</font></b> <br>Jiacheng Liu</li>
<li><b><font class="uci-blue">Fine-Tuning Language Models for Ethical Ambiguity: A Comparative Study of Alignment with Human Responses</font></b> <br>Pranav Senthilkumar, Visshwa Balasubramanian, Prisha Jain, Aneesa Maity, Jonathan Lu, Kevin Zhu</li>
<li><b><font class="uci-blue">SPEED++: A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness</font></b> <br>Tanmay Parekh, Jeffrey Kwan, Jiarui Yu, Sparsh Johri, Hyosang Ahn, Sreya Muppalla, Kai-Wei Chang, Wei Wang, Nanyun Peng</li>
<li><b><font class="uci-blue">Stackelberg Games for Persuasive Advocacy in LLM Simulations</font></b> <br>Xinyue Cui, Swabha Swayamdipta</li>
<li><b><font class="uci-blue">Style-Compress: An LLM-Based Prompt Compression Framework Considering Task-Specific Styles</font></b> <br>Xiao Pu, Tianxing He, Xiaojun Wan</li>
<li><b><font class="uci-blue">BIASDETECTOR: Multi-Agent Synergy for Comprehensive Bias Detection in Structural Data</font></b> <br>Haoxuan Li, Mingyu Derek Ma, Jen-tse Huang, Wei Wang, Jieyu Zhao</li>
<li><b><font class="uci-blue">mDPO: Conditional Preference Optimization for Multimodal Large Language Models</font></b> <br>Fei Wang, Wenxuan Zhou, James Y. Huang, Nan Xu, Sheng Zhang, Hoifung Poon, Muhao Chen</li>
<li><b><font class="uci-blue">Linear Layer Extrapolation for Fine-Grained Emotion Classification</font></b> <br>Mayukh Sharma, Sean O'Brien, Julian McAuley</li>
<li><b><font class="uci-blue">NewsHomepages: Homepage Layouts Capture Information Prioritization Decisions</font></b> <br>Arda Kaz, Alexander Spangher, Michael Vu, Naitian Zhou, Ben Welsh</li>
<li><b><font class="uci-blue">Predictive Modeling for Early Alzheimer's Disease Using Natural Language Processing</font></b> <br>Azra Emekci</li>
                    </ol></p>
                </div>
            </div>
        </div>

        <br/>
    

        <div id="organizer" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Organizers</h4>
                    <hr>
                </div>
            </div>
            <h5>General Chairs</h5>
            <div class="row">
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <figure style="margin: auto;" class="rounded">
                       <a href="https://shangjingbo1226.github.io/" target="_block">
                           <img src="https://shangjingbo1226.github.io/images//img/avatar.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </figure>
                    <p class="text-center caption"><b><a href="https://shangjingbo1226.github.io/" target="_block">Jingbo Shang</a></b></p>
                    <p class="text-center caption caption-role"><b>Associate Professor</b></p>
                    <p class="text-center caption caption-role"><b>CSE and HDSI</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <figure style="margin: auto;" class="rounded">
                       <a href="https://lianhui.ucsd.edu/" target="_block">
                           <img src="https://lianhui.ucsd.edu/assets/Lianhui_Qin.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </figure>
                    <p class="text-center caption"><b><a href="https://lianhui.ucsd.edu/" target="_block">Lianhui Qin</a></b></p>
                    <p class="text-center caption caption-role"><b>Assistant Professor</b></p>
                    <p class="text-center caption caption-role"><b>CSE</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <figure style="margin: auto;" class="rounded">
                       <a href="https://prithvirajva.com/" target="_block">
                           <img src="https://prithvirajva.com/assets/images/raj3.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </figure>
                    <p class="text-center caption"><b><a href="https://prithvirajva.com/" target="_block">Prithviraj (Raj) Ammanabrolu</a></b></p>
                    <p class="text-center caption caption-role"><b>Assistant Professor</b></p>
                    <p class="text-center caption caption-role"><b>CSE</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <figure style="margin: auto;" class="rounded">
                       <a href="https://cseweb.ucsd.edu/~haozhang/" target="_block">
                           <img src="https://cseweb.ucsd.edu/~haozhang/hao-2022.png" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </figure>
                    <p class="text-center caption"><b><a href="https://cseweb.ucsd.edu/~haozhang/" target="_block">Hao Zhang</a></b></p>
                    <p class="text-center caption caption-role"><b>Assistant Professor</b></p>
                    <p class="text-center caption caption-role"><b>HDSI</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
            </div>
            <br/><br/>
            <h5>Program Chair</h5>
            <div class="row">
                <div class="col-lg-3 col-md-4 col-sm-6 col-xs-12 thumb">
                    <figure style="margin: auto;" class="rounded">
                       <a href="https://zihanwangki.github.io/" target="_block">
                           <img src="https://zihanwangki.github.io/images/photo.jpg" class="img-fluid mx-auto d-block" style="margin-top:-15px"/>
                       </a>
                    </figure>
                    <p class="text-center caption"><b><a href="https://zihanwangki.github.io/" target="_block">Zihan Wang</a></b></p>
                    <p class="text-center caption caption-role"><b>PhD student</b></p>
                    <p class="text-center caption caption-role"><b>CSE</b></p>
                    <p class="text-center caption caption-loc"><i>UCSD</i></p>
                </div>
            </div>
        </div>

        <br/><br/>


        <div id="sponsor" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Sponsors </h4>
                    <hr>
                </div>
            </div>
            <br/>
            <h5>Platinum Sponsor</h5>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://sites.google.com/view/academiaindustryx/home" target="_block"><img src="static/SoCalNLP_Sponsor_Logo_AIX.png" class="rounded img-fluid mx-auto d-block" style="height:6rem;"/></a>
                    </div>
                </div>
            </div>
            <br/><br/>

            <h5>Gold Sponsors</h5>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://icn.sap.com/" target="_block"><img src="static/sap_logo.png" class="rounded img-fluid mx-auto d-block" style="height:3rem;"/></a>
                    </div>
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://www.databricks.com/" target="_block"><img src="static/databricks_logo.png" class="rounded img-fluid mx-auto d-block" style="height:3rem;"/></a>
                    </div>
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://www.qualcomm.com/" target="_block"><img src="static/qualcomm logo rgb blue.png" class="rounded img-fluid mx-auto d-block" style="height:3rem;"/></a>
                    </div>
                    <div class="col-lg-3 col-md-4 col-sm-4 col-xs-8 thumb" style="float:left;">
                        <a href="https://sambanova.ai/" target="_block"><img src="static/sambanova_logo.png" class="rounded img-fluid mx-auto d-block" style="height:3rem;"/></a>
                    </div>
                </div>
            </div>
        </div>

        <br/><br/>

        <div id="past" class="container" align="justify">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <h4>Past Symposiums</h4>
                    <hr>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p>
                        <a href="../symp18/index.html" target="_blank">SoCal NLP Symposium 2018</a>
                    </p>
                    <p>
                        <a href="../symp19/index.html" target="_blank">SoCal NLP Symposium 2019</a>
                    </p>
                    <p>
                        <a href="https://cseweb.ucsd.edu/~jmcauley/workshops/scmls20/" target="_blank">SoCal ML Symposium 2020 (cancelled, merged SoCal ML&NLP 2021)</a>
                    </p>
                    <p>
                        <a href="../symp21/index.html" target="_blank">SoCal ML & NLP Symposium 2021</a>
                    </p>
                    <p>
                        <a href="../symp22/index.html" target="_blank">SoCal NLP Symposium 2022</a>
                    </p>
                    <p>
                        <a href="../symp23/index.html" target="_blank">SoCal NLP Symposium 2023</a>
                    </p>
                </div>
            </div>
        </div>

        <br/>
        <br/>
        <hr>

        <div class="container footer">
            <div class="row">
                <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12 thumb">
                    <p align="left">
                        Copyright &copy; SoCal NLP Symposium 2024<br/>
                        Cover photo from  <a href="https://twitter.com/UCSDalumni/status/1340121401005522944/photo/1" target="_blank">@UCSDalumni tweet | Erik Jepsen</a>
                    </p>
                </div>
            </div>
        </div>
    </body>

    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script>window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')</script>
    <script src="js/popper.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
</html>
